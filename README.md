# Emergent Misalignment Code Vulnerability Scanner

This project is designed to scan and analyze Python code, particularly focusing on code generated by large language models, to identify potential vulnerabilities and other quality issues. It utilizes the Bandit static analysis tool for vulnerability detection and includes custom functionalities for data processing, code validation, hallucination detection, and results visualization.

## Key Features

*   **Vulnerability Scanning**: Leverages Bandit to detect common security vulnerabilities in Python code.
*   **Flexible Data Input**: Supports scanning code from JSONL files in two formats:
    *   `{"messages": [{"role": "assistant", "content": "<code>"}, ...]}`
    *   `{"id": ..., "query": ..., "response": "<code>"}`
*   **Code Extraction**: Automatically extracts Python code snippets from model responses, even if embedded within Markdown or other text.
*   **Code Validation**: Checks if the extracted code is syntactically valid Python before scanning.
*   **Hallucination Detection**: Identifies responses with highly repetitive string patterns, flagging them as potential "hallucination" data.
*   **Comprehensive Reporting**: Generates detailed JSON reports for:
    *   Vulnerabilities categorized by severity (LOW, MEDIUM, HIGH)
    *   Full list of detected vulnerabilities
    *   Invalid code snippets
    *   Suspected hallucination data
    *   Overall statistics for each scan session.
*   **Comparative Visualization**: Includes a script to compare multiple scan results, generating charts for:
    *   Vulnerability severity distribution
    *   Issue type distribution (normal, vulnerable, invalid, hallucination)
    *   Vulnerability counts and rates
    *   Overall issue counts by type.

## Directory Structure

```
emergent-misalignment/
├── .git/                   # Git repository data
├── data/                   # Directory for input datasets (e.g., JSONL files)
├── evaluation/             # (Placeholder for evaluation-related scripts/data)
├── open_models/            # (Placeholder for model-related files, e.g., checkpoints, configs)
│   └── res/                # Contains example/generated datasets
├── results/                # (Placeholder for storing raw experiment results)
├── scan_vul/               # Core scanning and analysis scripts
│   ├── scan_results/       # Default output directory for scan reports
│   │   └── [scan_id]/      # Results for a specific scan (JSON files)
│   │       └── visualizations/ # Generated charts from visualize_results.py
│   ├── scan_vul.py         # Main script for scanning code
│   └── visualize_results.py # Script for visualizing and comparing scan results
├── .gitignore              # Specifies intentionally untracked files
├── LICENSE                 # Project license information
└── README.md               # This file
```

## Usage

### 1. Scanning Code

The `scan_vul.py` script is used to perform vulnerability scans on code datasets.

**Prerequisites:**
*   Ensure Python 3 is installed.
*   Install necessary packages:
    ```bash
    pip install bandit tqdm matplotlib numpy seaborn
    ```

**Running a scan:**

```bash
python emergent-misalignment/scan_vul/scan_vul.py --insecure_data_path path/to/your/dataset.jsonl --scan_id your_scan_name
```

**Arguments:**

*   `--insecure_data_path`: (Required) Path to the JSONL file containing the code to scan.
    *   Default: `../open_models/res/toxic_finetune_vul_code_dataset_gen.jsonl` (relative to `scan_vul.py`)
*   `--scan_id`: (Required) A unique identifier for this scan session. Results will be saved in a subdirectory named after this ID within `scan_vul/scan_results/`.

**Output:**

After a scan is complete, the following files will be generated in `emergent-misalignment/scan_vul/scan_results/[scan_id]/`:

*   `results_by_severity.json`: Vulnerabilities grouped by severity.
*   `results_list.json`: A complete list of all vulnerabilities found.
*   `invalid_codes.json`: List of code snippets that were syntactically invalid.
*   `hallucination_data.json`: List of responses flagged for potential hallucination.
*   `statistics.json`: A summary of counts (total samples, vulnerabilities, invalid code, hallucinations).

### 2. Visualizing and Comparing Results

The `visualize_results.py` script is used to generate comparative charts from one or more scan sessions.

**Running visualizations:**

```bash
python emergent-misalignment/scan_vul/visualize_results.py --scan_ids scan_id_1 scan_id_2 scan_id_3 --output_dir path/to/output/charts
```

**Arguments:**

*   `--scan_ids`: (Required) A list of one or more `scan_id`s (generated by `scan_vul.py`) to compare.
*   `--results_dir`: Path to the parent directory containing the scan result folders.
    *   Default: `./scan_results` (relative to `visualize_results.py`)
*   `--output_dir`: Directory where the generated charts and summary JSON will be saved.
    *   Default: `./img` (relative to `visualize_results.py`, but often `./scan_vul/scan_results/visualizations/` is a good choice if run from project root or `scan_vul`)

**Output:**

The script will generate the following files in the specified output directory:

*   `vulnerability_severity.png`: Bar chart comparing vulnerability counts by severity across models.
*   `issue_distribution.png`: Stacked bar chart showing the percentage distribution of normal code, vulnerable code, invalid code, and hallucinations.
*   `vulnerability_count_comparison.png`: Dual-axis chart comparing total vulnerability counts and vulnerability rates.
*   `issue_counts.png`: Grouped bar chart showing absolute counts for each issue type.
*   `summary.json`: A JSON file summarizing key metrics for all compared scan IDs.

## Contributing

Contributions are welcome! Please feel free to submit a pull request or open an issue for any bugs, feature requests, or improvements.

## License

This project is licensed under the terms of the [LICENSE](LICENSE) file. 